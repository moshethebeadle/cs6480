{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Jacob (Jake) Toronto\n",
    "\n",
    "10991530\n",
    "\n",
    "Spring 2024\n",
    "\n",
    "CS 6480 Advanced Machine Learning\n",
    "\n",
    "Dr. Larry Zeng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Suggest a combination of perceptrons to represent the XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume two inputs that are real numbers: $a_1$, $a_2$ $\\in$ $\\real$, i.e. ($-\\infty$, $+\\infty$)\n",
    "\n",
    "The goal is to make a multi-layer perceptron that does the following:\n",
    "\n",
    "1. Convert the real-valued inputs into binary inputs ($0$ or $1$) using the $step$ function, defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "step(a,b) = \n",
    "\\left\\{\n",
    "    \\begin{array}{lr}\n",
    "        0, & \\text{if } i \\leq 0\\\\\n",
    "        1, & \\text{if } i\\gt 0\n",
    "    \\end{array}\n",
    "\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "2. Generate output equivalent to the $XOR$ function. A single perceptron cannot model the $XOR$ function. However, perceptrons can model $AND$, $OR$, and $NOT$.  By combining these together into a multi-layer perceptron, we can model $XOR$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $XOR$ in terms of $AND$, $OR$ and $NOT$\n",
    "\n",
    "We can define $XOR$ in terms of other logical functions:\n",
    "\n",
    "$$XOR = AND(OR(x_1, x_2), NOT(AND(a_1, a_2)))$$\n",
    "\n",
    "Another way to describe $NOT(AND)$ is to simply call it $NAND$ and have it be the reverse output of $AND$.\n",
    "\n",
    "This gives:\n",
    "\n",
    "$$XOR = AND(OR(a_1, a_2), NAND(a_1, a_2))$$\n",
    "\n",
    "Please review this [Google Sheet that has truth tables showing this relationship](https://docs.google.com/spreadsheets/d/1NR9ilyLD3BuZpmpSZyWDZGX4EjgCfxOxltJ3-xPDbFQ/edit?usp=sharing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $AND$, $OR$, and $NAND$ in terms of perceptrons\n",
    "\n",
    "The same [Google Sheet](https://docs.google.com/spreadsheets/d/1NR9ilyLD3BuZpmpSZyWDZGX4EjgCfxOxltJ3-xPDbFQ/edit?usp=sharing) also shows how perceptrons can model these three binary functions.\n",
    "\n",
    "Note, a perceptron is the composition of an activation function and a linear combination of weights and inputs with a bias.\n",
    "\n",
    "In our case, it is as follows:\n",
    "- the activation function is the $step$ function\n",
    "- since we have two inputs, the linear combination would look like this: $w_1a_1 + w_2a_2 + b$, where $w_n$ are the weights, $a_n$ are the inputs, and $b$ is the bias\n",
    "\n",
    "To summarize the info from the spreadsheet, we define each of the basic logical functions as follows:\n",
    "\n",
    "### $AND$\n",
    "\n",
    "$$AND(a_1, a_2) = step(a_1 + a_2 - 0.5)$$\n",
    "\n",
    "### $OR$\n",
    "\n",
    "$$OR(a_1, a_2) = step(a_1 + a_2 - 1.5)$$\n",
    "\n",
    "### $NAND$\n",
    "\n",
    "$$NAND(a_1, a_2) = step(-a_1 - a_2 + 1.5)$$\n",
    "\n",
    "(Note: these assume that $a_1$ and $a_2$ are binary inputs $\\in$ ($0, 1$))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "With that background, we can model the multi-layer perceptron as a series of equations, with three layers.\n",
    "\n",
    "The outputs from one layer lead into the next, as you will see by the reuse of the variables.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "The inputs are $a_1$ and $a_2$.  They are real-valued.\n",
    "\n",
    "### Layer 1\n",
    "\n",
    "This is the layer that converts real-valued inputs into binary.\n",
    "\n",
    "$$b_1 = step(a_1)$$\n",
    "\n",
    "$$b_2 = step(b_2)$$\n",
    "\n",
    "### Layer 2\n",
    "\n",
    "This layer implements the $NAND$ and $OR$\n",
    "\n",
    "$$c_1 = step(b_1 + b_2 - 0.5)$$\n",
    "\n",
    "$$c_2 = step(-b_1 - b_2 + 1.5)$$\n",
    "\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "This layer finishes it up by $AND$-ing the previous layer's results\n",
    "\n",
    "$$d = step(c1 + c2 - 1.5)$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
